\documentclass[a4paper]{report}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{natbib}
\usepackage{color}

%Inserting codes
\definecolor{green}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset
{
        frame = tb,
        language = C,
        aboveskip = 3mm,
        belowskip = 3mm,
        showstringspaces = false,
        columns = flexible,
        basicstyle = {\small\ttfamily},
        numberstyle = \tiny\color{gray},
        keywordstyle = \color{blue},
        commentstyle = \color{green},
        stringstyle = \color{mauve},
        breaklines = true,
        breakatwhitespace = true,
        tabsize = 4,
        numbers = left,
        stepnumber = 1,
        numberfirstline = true
}

\begin{document}

\title{\Huge{Extended Essay} \\[2cm] \Large{Fast Fourier Transform Implementation on X86 Architecture with SIMD Optimization} \\[1cm]}
\author{Kenton Hua \\[0.5cm] Shanghai Pinghe Bilingual School}
\date{August 14, 2014}
\maketitle

\newpage

\chapter*{Abstract} \indent

	Fast Fourier Transform(FFT), as an optimized algorithm and discrete version derived from Fourier Transform(FT), is widely used in the field of Engineering and Computer Science. In most occasions, such transform can be understood as a change from time domain to frequency domain on a series of signal; the Fourier Transform of a time domain signal completely describes the frequency components of the signal.

	Typical use of FFT involves data compression, audio/image processing, speech synthesis/recognization, digital communication and many other important applications.
	
	The purpose of this Extended Essay is to present, examine, and summarize several optimization techniques in programming by implementating a highly optimized single-precision FFT routine library, which is named ee-fft. This library performs an 1024-point FFT in 0.01 millisecond on an Intel T7250 mobile processor, which is 70 percent of speed performance of the world's fastest implementation.

	My choice of CPU architecture is x86 with SSE2 instruction set support since a vast majority of today's personal computer are compatible with such instruction sets. In first chapter I will start from listing several optimization techniques available on the chosen platform; chapter 2 will start with a quick introduction of FFT and outline the structure of ee-fft; in chapter 3 I will describe each part of the implementation in detail; benchmark, evaluation, comparison and conclusion will be presented in chapter 4.

\newpage
\tableofcontents

\newpage

\chapter{Optimization Techniques on X86 with SIMD Support} \indent

	X86 processor family involves a range of models from the earliest Intel 8086 to the newest Intel Haswell, Xeon and compatible AMD series. In this essay X86 refers to 32 or 64-bit X86 processors with at least FPU, MMX, SSE and SSE2 instruction set support, which are the most common models in modern personal computers.
	
	More specifically, this essay targets at Intel X86 processors. But the instruction sets and other hardware features are compatible with most X86 processors made by other vendors(such as AMD and VIA Technologies).
	
	In this chapter the optimization techniques introduced below will be categorized by related hardware features.

\section{IA-32 Architecture} \indent

	IA-32 is the abbreviation of Intel Architecture 32-bit and it originated from Intel 80386 processor. IA-32 is the basic subset of and supported by all 32 or 64-bit x86 processors nowadays.

	Although this essay focus more on optimizations based on extended features of modern processors, it is necessary to briefly introduce IA-32 architecture as a basic processor model.

\subsection{Registers} \indent

	Registers in IA-32 Architecture consist of 8 general purpose registers(EAX, EBX, ECX, EDX, EDI, ESI, EBP, and ESP), 6 segment registers(CS, DS, SS, ES, FS, and GS), one flag register(EFLAGS), and one instruction pointer(EIP). Each general purpose register holds 32 bits of data for arithmetic or memory operation; each segment register holds 16 bits of data for memory management. (Intel, 3-10 Vol.1)

\subsection{Instruction Set} \indent

	IA-32 instruction sets offer basic operations, for example, data transfer, program control, integer arithmetic, and other operations, on memory and general purpose registers. (Intel, 5-3 Vol.1)
	
	However, IA-32 does not provide floating point operations, which are implemented in i486 and later architectures with FPU instruction set.

\section{Hardware Features and Optimizations} \indent

\subsection{SIMD(Single Instruction Multiple Data)} \indent

	Most IA-32 instructions operate on single data unit. For example, assembly code ``mov \$0xffffffff, \%eax" loads one 32-bit immediate value into EAX register.
	
	By contrast, SIMD(Single Instruction Multiple Data) instruction sets enable parallel operations on multiple data units during the execution of one instruction. Thus speed up the processing of data.

	MMX is the first SIMD instruction set added as an IA-32 extension for packed integer arithmetics. Another major extension is SSE(Streaming SIMD Extensions), first introduced since Intel Pentium III, providing packed addition/multiplication/shuffling to up to 4 single-precison floating point values. Later, SSE2 extended such capability to integers and double-precision floating point values. Subsequent updates include SSE3, SSSE3, SSE4, AVX, FMA and AVX2, which are less common and will not be further discussed in this essay.

	To expand storage space for intermediate values, SSE added eight 128-bit registers: XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, and XMM7. In 64-bit mode these are further expanded to XMM15.

\paragraph{Vectorization} \indent \bigskip

	To vectorize means to complete a computation in parallel, i.e., multiple data are processed each time.
	
	In many situations, we compute the element-wise multiplication of two array of floating point values. The typical C implementation would be:
	
        \lstset{language = c, tabsize = 4}
        \begin{lstlisting}
int i;
for(i = 0; i < N; i ++)
	dst[i] = src1[i] * src2[i];
        \end{lstlisting}

	By default many compilers will compile the codes with FPU instruction set which loads two numbers, perform a multiplication and store the result during each loop cycle. The following code is the vectorized version using gcc-style inline assembly and SSE instruction set:
	
        \lstset{language = c, tabsize = 4}
        \begin{lstlisting}
int i;
/* 4 multiplications each time */
for(i = 0; i < N - 3; i += 4)
	__inline__ __asm__
	(
		"movaps %1, %%xmm0\n" //load 4 floats from src1 to xmm0
		"mulps  %2, %%xmm0\n" //multiply xmm0 by floats from src2
		"movaps %%xmm0, %0\n" //store xmm0 into dst
		: "+m"(dst[i]) : "m"(src1[i]), "m"(src2[i]) : "%xmm0"
	);
/* Multiplications for the rest that do not fit in fours. */
for(; i < N; i ++)
	dst[i] = src1[i] * src2[i];
        \end{lstlisting}

	The vectorized code multiplies four float values in three commands and most SSE instructions can be completed in one processor cycle(if load/store time were to be neglected).

	One thing to notice is that ``movaps" instruction only accepts memory locations aligned by 16 bytes. Otherwise it should be replaced with ``movups" with a slight increase in latency.

\paragraph{Replacing FPU instructions} \indent \bigskip

	Even for scalar arithmetics, i.e., single data is processed by each instruction, SSE is faster than equivalent FPU instructions. So SSE instructions for scalar operations are suggested to replace the FPU ones in most ocassions.

\subsection{Cache} \indent

	Caches are buffers in CPUs for speeding up access to frequently used memory regions.
	
	Direct memory access incurs a latency of hundreds of processor cycles. But when frequently used memory units are loaded, processed, and stored in caches, the latency can be shortened by tens or even hundreds.
	
	To speed up data transfer between cache and memory, multiple levels of cascaded caches are introduced to modern processors. Sorted by number, the cache directly accessed by the processor is called L1, and the one accessed by L1 is L2, then L3, etc.
	
	In today's mainstream processors, each core has a 32 or 64KB L1 cache, and a 256KB L2 cache. Some models has L3 and even L4 caches with a capacity of several mega bytes. The L1 cache is further segmented into instruction cache and data cache.

\paragraph{Cache Optimization} \indent \bigskip

	When an instruction accesses to a memory unit beyond the cache, a cache miss takes place. In such situations the processor has to pause execution until the memory content is loaded into the cache.
	
	Programmers have to optimize the codes to minimize cache misses, especially when processing over large amount of data. Consider the following senario:
	
	After some processing procedures, two very large array of float values are multiplied and the results are stored in another array. Such process is repeated by thousands of times in each second.
	
	Typical C implementation without cache optimization would be:

        \lstset{language = c, tabsize = 4}
        \begin{lstlisting}
int i, j;
for(i = 0; i < N_repeat; i ++)
{
	/*
		... Some data processing ...
	*/
	for(j = 0; j < N_datasize; j ++)
		dst[i] = src1[i] * src2[i];
	/*
		... Some further processing ...
	*/
}
        \end{lstlisting}

	The above codes can be much slower when N\_datasize exceeds the size of L1 data cache. An optimized version can be:

        \lstset{language = c, tabsize = 4}
        \begin{lstlisting}
/* this function definition should be in the file scope */
static void process(float* dst, float* src1, float* src2, int size)
{
	int j;
	for(j = 0; j < N_repeat; j ++)
	{
		/*
			... some data processing ...
		*/
		for(j = 0; j < size; j ++)
			dst[i] = src1[i] * src2[i];
		/*
			... some further processing ...
		*/
	}
}

int i;
/* repeating the loop between cache refreshes */
for(i = 0; i < N_datasize - L1data_size + 1; i += L1data_size)
	process(dst + i, src1 + i, src2 + i, L1data_size);

/* process the rest that do not fit in cache sizes */
process(dst + i, src1 + i, src2 + i, N_datasize - i);
        \end{lstlisting}

\subsection{Instruction Pipeline} \indent

	The following steps outline the five stage model for the operation of one instruction:
	
	\begin{enumerate}
		\item Fetch the instruction bytes from cache.
		\item Decode the instruction.
		\item Execute the instruction.
		\item Load data need by the instruction from cache.
		\item Write the result back into cache.
	\end{enumerate}

	On processors with instruction pipelines, the five steps are respectively completed by individual parts of the circuit. Theoretically up to five instructions can be processed simutaneously, if the five parts are used for successive instructions on different stages at the same time. So pipelining does not shorten the latency(in some ocassions it extends the latency) but increases the throughput of instructions.

	%TODO: I need a picture here.

	One problem is, when an instruction depends on the result of previous instruction execution, the later instruction has to be delayed to retain the processing sequence. Otherwise an incorrect result is yielded, which is called a hazard.

\subsubsection{Execution Core} \indent

	Many latest CPUs has mutiple fetchers, decoders and even execution units. The decoders break down instructions into a chain of micro operations(or micro-ops), which are smaller and more specific instructions. The execution, data load and store parts are combined into an Execution Core, inside which are multiple ports that can execute different micro-ops in parallel. For instance, each Execution Core in Intel Core processors has six ports: data mover between registers, adder, multiplier, shifter/shuffler, data loader, and data storer. (Opt Manual, 2-14)

\subsubsection{Minimizing Dependences} \indent

	We should notice that careful sequencing of codes does make a difference on execution time, comparing to those unarranged codes even with exactly same instructions.
	
	\begin{enumerate}
	
	\item The dependency on the same data by neighboring instructions should be minimized.
	
	\item The dependency on the same execution port by neighboring instructions should be minimized.

	\end{enumerate}
\subsection{Branch Prediction} \indent

	Branch Prediction is a technique used to maximize the usage of pipeline. Traditional pipeline fails to run parallel when a conditional or unconditional jump or call is met. To fix this problem, a Branch Prediction Unit(BPU), which predicts the destination of incoming jumps, calls and returns, is added to the instruction fetcher. (Opt Manual, 2-26)
	
\subsubsection{Loop Unrolling} \indent
	
	When BPU fails to predict a branch, the pipelines would be slowed down. One effective way to solve this problem is to reduce the branches if possible. One more specific yet common method is called loop unrolling. The following code is an example of loop-unrolled version of the multiplication of two float arrays:
	
        \lstset{language = c, tabsize = 4}
        \begin{lstlisting}
int i;
/* 4 multiplications each time */
for(i = 0; i < N - 3; i += 4)
{
	dst[i] = src1[i] * src2[i];
	dst[i + 1] = src1[i + 1] * src2[i + 1];
	dst[i + 2] = src1[i + 2] * src2[i + 2];
	dst[i + 3] = src1[i + 3] * src2[i + 3];
}
/* Multiplications for the rest that do not fit in fours. */
for(; i < N; i ++)
	dst[i] = src1[i] * src2[i];
        \end{lstlisting}

	When compiling C programs with an optimization flag, many compilers can automatically unroll the loops but in some ocassions manually unroll the loops yields faster programs, especially when combining loop unrolling with vectorization.

	There is a trade-off between loop unrolling with cache miss rate. In the multiplication case, if N equals to 20000 and the loop is unrolled into two iterations of 10000 multiplications, the size of compiled binary may become larger than L1 instruction cache. So the optimized code may be even slower than the original one.

\chapter{FFT Derivation and Program Structure}
\chapter{The Implentation}
\chapter{Evaluation and Review}

\appendix
\chapter{Source Code}

\end{document}

